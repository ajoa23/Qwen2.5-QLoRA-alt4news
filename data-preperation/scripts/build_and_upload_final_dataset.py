import os
from datasets import load_dataset
from huggingface_hub import login
from dotenv import load_dotenv
from pathlib import Path

# 1Ô∏è‚É£ Load environment variables and Hugging Face token
load_dotenv()
HF_TOKEN = os.getenv("HF_TOKEN")
assert HF_TOKEN, "Please set your Hugging Face token in the .env file as HF_TOKEN."

# 2Ô∏è‚É£ Configurations
DATASET_NAME_RAW = "Alex23o4/n24news_sample_synthetic_alttext"
DATASET_NAME_REDUCED = "Alex23o4/n24news_sample_synthetic_alttext_reduced"
LOCAL_SAVE_PATH = "processed/n24news_reduced"

# Hugging Face Login
login(token=HF_TOKEN)

# 3Ô∏è‚É£ Load Full Dataset
print("üì• Loading full dataset...")
raw_ds = load_dataset(DATASET_NAME_RAW)

# 4Ô∏è‚É£ Fields to Retain
FIELDS_TO_KEEP = [
    "abstract",
    "caption",
    "headline",
    "image_id",
    "openai_alt_text_refined",
    "section",
    "section_label",
    "image_url_clean",
]

# 5Ô∏è‚É£ Transformation Function to Reduce Fields
def reduce_example(example):
    return {key: example[key] for key in FIELDS_TO_KEEP}

print("üßπ Reducing dataset...")
reduced_ds = raw_ds.map(
    reduce_example,
    remove_columns=raw_ds["train"].column_names  # Remove all other columns
)

# 6Ô∏è‚É£ Show Example
print("\n‚úÖ Example after reduction:")
print(reduced_ds["train"][0])

# 7Ô∏è‚É£ (Optional) Save Locally
if LOCAL_SAVE_PATH:
    print(f"üíæ Saving dataset locally to {LOCAL_SAVE_PATH}...")
    os.makedirs(LOCAL_SAVE_PATH, exist_ok=True)
    reduced_ds.save_to_disk(LOCAL_SAVE_PATH)

# 8Ô∏è‚É£ (Optional) Push to Hugging Face Hub
push_to_hub = True  # Set to False if you don't want to push

if push_to_hub:
    print(f"üöÄ Pushing reduced dataset to Hugging Face Hub as '{DATASET_NAME_REDUCED}'...")
    reduced_ds.push_to_hub(DATASET_NAME_REDUCED)

print("\nüéâ All done! Reduced dataset is ready.")
